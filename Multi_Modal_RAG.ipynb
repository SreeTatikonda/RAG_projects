{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdlG+TGFD3PiWhjYf+3kmL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SreeTatikonda/RAG_projects/blob/main/Multi_Modal_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GEZTbjDR3BX"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y openjdk-21-jdk\n",
        "!update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-21-openjdk-amd64/bin/java 1\n",
        "!update-alternatives --config java\n",
        "!java -version\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Basic setup\n",
        "!pip -q install pymupdf pdfplumber pytesseract sentence-transformers faiss-cpu pyserini pillow transformers accelerate bitsandbytes\n",
        "import os, json, math, re, io\n",
        "import fitz  # PyMuPDF\n",
        "import pdfplumber\n",
        "from PIL import Image\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Choose small, free models\n",
        "TEXT_EMB = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
        "CAPTION_EMB = SentenceTransformer(\"intfloat/e5-small\")\n",
        "# For generation, weâ€™ll select later based on GPU\n"
      ],
      "metadata": {
        "id": "1rcKD3Z_SKAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Parse PDF into text sections and figure crops with captions\n",
        "def parse_pdf(pdf_path, dpi=200):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages = []\n",
        "    for i in range(len(doc)):\n",
        "        page = doc[i]\n",
        "        text = page.get_text(\"blocks\")  # preserves layout blocks\n",
        "        pages.append({\"index\": i, \"blocks\": text})\n",
        "    return doc, pages\n",
        "\n",
        "def extract_figures_with_captions(pdf_path):\n",
        "    figures = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for pi, page in enumerate(pdf.pages):\n",
        "            # Heuristic: captions often start with \"Figure\" or \"Fig.\"\n",
        "            text = page.extract_text() or \"\"\n",
        "            # Simple heuristic: find lines starting with Fig/Figure\n",
        "            caption_candidates = [l for l in (text.split(\"\\n\")) if re.match(r\"^\\s*(Fig\\.|Figure)\\s*\\d+\", l)]\n",
        "            # Rough image detection via page.images (bbox)\n",
        "            for im in page.images:\n",
        "                # Crop image\n",
        "                bbox = (im[\"x0\"], im[\"top\"], im[\"x1\"], im[\"bottom\"])\n",
        "                crop = page.crop(bbox).to_image(resolution=200)\n",
        "                img_bytes = io.BytesIO()\n",
        "                crop.save(img_bytes, format=\"PNG\")\n",
        "                img_bytes.seek(0)\n",
        "                # Attach nearest caption line (fallback empty)\n",
        "                cap = caption_candidates[0] if caption_candidates else \"\"\n",
        "                figures.append({\n",
        "                    \"page\": pi, \"bbox\": bbox, \"caption\": cap, \"image_bytes\": img_bytes.getvalue()\n",
        "                })\n",
        "    return figures\n"
      ],
      "metadata": {
        "id": "JF-futtqUFE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Build text chunks with overlap and link figure callouts\n",
        "def blocks_to_paragraphs(pages):\n",
        "    paras = []\n",
        "    pid = 0\n",
        "    for p in pages:\n",
        "        # Join block text, keep page index\n",
        "        page_text = \"\\n\".join([b[4] for b in p[\"blocks\"] if isinstance(b[4], str) and b[4].strip()])\n",
        "        # Split by double newline as a crude paragraph boundary\n",
        "        for para in re.split(r\"\\n{2,}\", page_text):\n",
        "            para = para.strip()\n",
        "            if len(para) > 0:\n",
        "                paras.append({\"paragraph_id\": pid, \"page\": p[\"index\"], \"text\": para})\n",
        "                pid += 1\n",
        "    return paras\n",
        "\n",
        "def make_chunks(paras, min_tokens=180, max_tokens=600):\n",
        "    # Token proxy: words count\n",
        "    chunks = []\n",
        "    buf = []\n",
        "    wcount = 0\n",
        "    for para in paras:\n",
        "        words = para[\"text\"].split()\n",
        "        if wcount + len(words) > max_tokens and buf:\n",
        "            chunks.append({\n",
        "                \"text\": \" \".join([b[\"text\"] for b in buf]),\n",
        "                \"pages\": list({b[\"page\"] for b in buf}),\n",
        "                \"paragraph_ids\": [b[\"paragraph_id\"] for b in buf]\n",
        "            })\n",
        "            # start new buffer with overlap\n",
        "            overlap = buf[-1:]\n",
        "            buf = overlap + [para]\n",
        "            wcount = sum(len(b[\"text\"].split()) for b in buf)\n",
        "        else:\n",
        "            buf.append(para)\n",
        "            wcount += len(words)\n",
        "    if buf:\n",
        "        chunks.append({\n",
        "            \"text\": \" \".join([b[\"text\"] for b in buf]),\n",
        "            \"pages\": list({b[\"page\"] for b in buf}),\n",
        "            \"paragraph_ids\": [b[\"paragraph_id\"] for b in buf]\n",
        "        })\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "fgHlWTYtUJA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "REqGFwICU3TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y openjdk-21-jdk\n",
        "!update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-21-openjdk-amd64/bin/java 1\n",
        "!update-alternatives --config java\n",
        "!java -version\n"
      ],
      "metadata": {
        "id": "hhYI_0o6U4J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25\n"
      ],
      "metadata": {
        "id": "xJbu7A37V7Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Embeddings + FAISS index (dense) and BM25 (sparse)\n",
        "import faiss, numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "def build_dense_index(chunks, model=TEXT_EMB):\n",
        "    texts = [c[\"text\"] for c in chunks]\n",
        "    vecs = model.encode(texts, normalize_embeddings=True)\n",
        "    index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "    index.add(np.array(vecs).astype(np.float32))\n",
        "    return index, vecs, texts\n",
        "\n",
        "def build_caption_index(figures, model=CAPTION_EMB):\n",
        "    caps = [f[\"caption\"] or \"\" for f in figures]\n",
        "    vecs = model.encode(caps, normalize_embeddings=True)\n",
        "    index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "    index.add(np.array(vecs).astype(np.float32))\n",
        "    return index, vecs, caps\n",
        "\n",
        "def build_bm25(chunks):\n",
        "    tokenized = [c[\"text\"].split() for c in chunks]\n",
        "    bm25 = BM25Okapi(tokenized)\n",
        "    return bm25, tokenized\n",
        "\n",
        "# Correct usage (unpack the tuples properly)\n",
        "text_index, text_vecs, text_texts = build_dense_index(chunks)\n",
        "cap_index, cap_vecs, cap_texts   = build_caption_index(figs)\n",
        "bm25, tokenized                  = build_bm25(chunks)\n"
      ],
      "metadata": {
        "id": "cO-5tGx2ULg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build indices\n",
        "text_index, text_vecs, text_texts = build_dense_index(chunks)\n",
        "cap_index, cap_vecs, cap_texts = build_caption_index(figs)\n",
        "bm25, tokenized = build_bm25(chunks)\n",
        "\n",
        "# Example query\n",
        "query = \"cloud computing adoption trends\"\n",
        "q_vec = TEXT_EMB.encode([query], normalize_embeddings=True).astype(np.float32)\n",
        "\n",
        "# Dense retrieval\n",
        "D, I = text_index.search(q_vec, k=5)\n",
        "dense_hits = [text_texts[i] for i in I[0]]\n",
        "\n",
        "# BM25 retrieval\n",
        "bm25_scores = bm25.get_scores(query.split())\n",
        "top_bm25 = np.argsort(bm25_scores)[::-1][:5]\n",
        "bm25_hits = [chunks[i][\"text\"] for i in top_bm25]\n"
      ],
      "metadata": {
        "id": "3eimKUQEZbNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Retrieval: text + figure captions + simple fusion\n",
        "def retrieve(query, chunks, figures, text_index, cap_index, topk=6, figk=4):\n",
        "    q_text_vec = TEXT_EMB.encode([query], normalize_embeddings=True).astype(np.float32)\n",
        "    D, I = text_index.search(q_text_vec, topk)\n",
        "    text_hits = [{\"idx\": int(i), \"score\": float(d), \"chunk\": chunks[int(i)]} for d, i in zip(D[0], I[0])]\n",
        "\n",
        "    q_cap_vec = CAPTION_EMB.encode([query], normalize_embeddings=True).astype(np.float32)\n",
        "    D2, I2 = cap_index.search(q_cap_vec, figk)\n",
        "    fig_hits = [{\"idx\": int(i), \"score\": float(d), \"figure\": figures[int(i)]} for d, i in zip(D2[0], I2[0])]\n",
        "\n",
        "    return text_hits, fig_hits\n"
      ],
      "metadata": {
        "id": "Jm8G3VxtUZ8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Visual QA (optional): extract key facts from figures\n",
        "# Using Qwen-VL or LLaVA for small QA over images\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def load_vlm(name=\"Qwen/Qwen2-VL-2B-Instruct\"):\n",
        "    processor = AutoProcessor.from_pretrained(name)\n",
        "    model = AutoModelForVision2Seq.from_pretrained(name, device_map=\"auto\", torch_dtype=\"auto\")\n",
        "    return processor, model\n",
        "\n",
        "def vqa_on_figures(fig_hits, question, processor, model):\n",
        "    facts = []\n",
        "    for fh in fig_hits:\n",
        "        img = Image.open(io.BytesIO(fh[\"figure\"][\"image_bytes\"])).convert(\"RGB\")\n",
        "        prompt = f\"Question: {question}\\nAnswer concisely with key values, trends, axes and units.\"\n",
        "        inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(model.device)\n",
        "        out = model.generate(**inputs, max_new_tokens=128)\n",
        "        ans = processor.batch_decode(out, skip_special_tokens=True)[0]\n",
        "        facts.append({\"page\": fh[\"figure\"][\"page\"], \"caption\": fh[\"figure\"][\"caption\"], \"answer\": ans})\n",
        "    return facts\n"
      ],
      "metadata": {
        "id": "Cy8KnX0YWN-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Generation: summary without chain-of-thought, with citations and figures\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def load_llm(name=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
        "    tok = AutoTokenizer.from_pretrained(name, use_fast=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        name, device_map=\"auto\", torch_dtype=\"auto\"\n",
        "    )\n",
        "    return tok, model\n",
        "\n",
        "def build_prompt(query, text_hits, fig_hits, vqa_facts):\n",
        "    # Assemble grounded context with locations\n",
        "    text_ctx = []\n",
        "    for h in text_hits:\n",
        "        pages = \", \".join([str(p) for p in h[\"chunk\"][\"pages\"]])\n",
        "        snippet = h[\"chunk\"][\"text\"][:1200]\n",
        "        text_ctx.append(f\"[TEXT p.{pages}] {snippet}\")\n",
        "\n",
        "    fig_ctx = []\n",
        "    for f in fig_hits:\n",
        "        fig_ctx.append(f\"[FIGURE p.{f['figure']['page']}] {f['figure']['caption']}\")\n",
        "\n",
        "    vqa_ctx = []\n",
        "    for v in vqa_facts:\n",
        "        vqa_ctx.append(f\"[FIGURE-FACT p.{v['page']}] {v['answer']}\")\n",
        "\n",
        "    system = (\n",
        "        \"You are a scientific assistant. Provide a concise summary answering the question. \"\n",
        "        \"Do not reveal your reasoning steps. Only use provided context. \"\n",
        "        \"Cite using page markers like [p.X] after each claim.\"\n",
        "    )\n",
        "    user = (\n",
        "        f\"Question: {query}\\n\\nContext:\\n\"\n",
        "        + \"\\n\".join(text_ctx[:6]) + \"\\n\"\n",
        "        + \"\\n\".join(fig_ctx[:4]) + \"\\n\"\n",
        "        + (\"\\n\".join(vqa_ctx) if vqa_ctx else \"\")\n",
        "        + \"\\n\\nInstructions:\\n\"\n",
        "        \"- Summarize without chain-of-thought.\\n\"\n",
        "        \"- Attribute claims with [p.PAGE].\\n\"\n",
        "        \"- If a figure supports a claim, cite it with [p.PAGE, figure].\"\n",
        "    )\n",
        "    return system, user\n",
        "\n",
        "def generate_answer(tok, model, system, user, max_new_tokens=400):\n",
        "    prompt = f\"<s>[SYSTEM]\\n{system}\\n[/SYSTEM]\\n[USER]\\n{user}\\n[/USER]\"\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "weMx25J9WQ18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGAWrZFOWirE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "pdf_path = list(uploaded.keys())[0]  # use the actual uploaded filename\n",
        "doc, pages = parse_pdf(pdf_path)\n",
        "paras = blocks_to_paragraphs(pages)\n",
        "chunks = make_chunks(paras)\n",
        "figs = extract_figures_with_captions(pdf_path)\n",
        "\n",
        "text_index, text_vecs, text_texts = build_dense_index(chunks)\n",
        "cap_index, cap_vecs, cap_texts   = build_caption_index(figs)\n",
        "\n",
        "\n",
        "query = \"What are the main findings and their quantitative results?\"\n",
        "text_hits, fig_hits = retrieve(query, chunks, figs, text_index, cap_index)\n",
        "\n",
        "# Optional VQA (skip if low GPU)\n",
        "# vlp, vlm = load_vlm()\n",
        "# vqa_facts = vqa_on_figures(fig_hits, query, vlp, vlm)\n",
        "vqa_facts = []\n",
        "\n",
        "tok, llm = load_llm()  # choose a small model if limited GPU\n",
        "system, user = build_prompt(query, text_hits, fig_hits, vqa_facts)\n",
        "answer = generate_answer(tok, llm, system, user)\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "Vr4UaYZ2WT9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}